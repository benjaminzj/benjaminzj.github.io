<!DOCTYPE html>
<html lang="en">




<head><meta name="generator" content="Hexo 3.8.0">

  <!-- Basic Page Needs
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta charset="utf-8">
  
      <title>Search - IZhaoJian's Blog</title>
  

  
  
  <meta name="description" content>
  <meta name="author" content="ZhaoJian">

  <!-- Mobile Specific Metas
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- load loadjs.js -->
  <script src="/libs/loadjs/dist/loadjs.min.js"></script>

<link rel="stylesheet" href="/libs/animate.css/animate.min.css">
  <!-- load lightgallery -->
<link rel="stylesheet" href="/css/lightgallery.css">
<link rel="stylesheet" href="/libs/noty/lib/noty.css">
<script src="https://code.jquery.com/jquery-3.3.1.min.js" integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin="anonymous"></script>
  






    <link rel="stylesheet" href="/css/taurus.css">
    
        <link rel="stylesheet" href="/css/scheme-taurus/animations.css">
    


<link rel="stylesheet" href="/.css">

  <!-- load font awesome 5 -->
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.2.0/css/all.css" integrity="sha384-hWVjflwFxL6sNzntih27bfxkr27PmbbK/iSvJ+a4+0owXq79v+lsFkW54bOGbiDQ" crossorigin="anonymous">

  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
  </script>
  <!-- load mathjax -->
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax//libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML"></script>

  <!-- load js-cookie -->
  <script src="https://cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js"></script>
    <script src="/js/social-share.min.js"></script>
    <script src="/js/theme.js"></script>

  <!-- include cookie.js -->
  
  

  <!-- include comment system code -->
  
  <!-- Favicon
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="icon" type="image/png" href="/images/favicon.png">
</head>
<body style="display: flex; flex-direction: column; min-height: 100vh;">

 

<header id="header" class="header">
	<div class="header-title">
		
		<div class="header-logo">
			<a href="/">
				<img src="/images/theme-icon.svg">
			</a>
		</div>
		<div class="header-text">
			<h1>
				<a href="/">IZhaoJian's Blog</a>
			</h1>
			<subtitle>
				極
			</subtitle>
		</div>
		
	</div>
	<div id="header-nav">
		



<nav id="nav">
	
	
		
			
		
		
			<div class="nav-item">
				
					<div class="nav-name">
				
					<a class="nav-link" href="/categories/项目/">
						<span>项目 </span>
					</a>
				</div>
			</div>
		
	
		
			
		
		
			<div class="nav-item">
				
					<div class="nav-name">
				
					<a class="nav-link" href="/categories/笔记/">
						<span>笔记 </span>
					</a>
				</div>
			</div>
		
	
	
	<div class="nav-item" id="nav-item-archive">
		
				<div class="nav-icon">
				
			<a href="/archives/" title="归档">
			<img src="/images/icons/blue-shadow/archive.svg" alt>
			</a>
		</div>
	</div>
	<div class="nav-item" id="nav-item-search">
		
		<div class="nav-icon active_dot">
		
			<a href="/search/" title="搜索">
			<img src="/images/icons/blue-shadow/search.svg" alt>
			</a>
		</div>
	</div>
	<div class="nav-item" id="nav-item-more">
		<div class="nav-icon">
				<a href="#" onclick="onClickMenuIcon(event);" ontouchstart="onClickMenuIcon(event);">
				<img src="/images/icons/blue-shadow/menu.svg" alt>
				</a>
		</div>
		<div class="nav-more-menu">
				<i class="far fa-times-circle" id="nav-more-menu-close" onclick="onClickNavMenuClose(event);" ontouchstart="onClickNavMenuClose(event);"></i>
		
			
			
				
			
		
			<div class="nav-more-item">
					<div class="nav-name">
						<a class="nav-link" href="/categories/项目/">
							<span>项目</span>
						</a>
					</div>
			</div>
		
		
			
			
				
					
				
			
		
		
		
		<div class="nav-more-item">
				<div class="nav-name">
					<a class="nav-link" href="/categories/笔记/">
						<span>笔记</span>
					</a>
				</div>
		</div>
		
	</div>
	</div>
</nav>

	</div>
</header>

 




  <!-- Primary Page Layout
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <div style="flex: 1;">
      <style>
    body {
        background-color: white;
    }
</style>
<div class="search-container">
	<input type="text" id="search-form">

	<ul class="cat-list">
		
			<li><a href="/categories/笔记/"><img src="/images/笔记.svg" alt="笔记" onerror="if(this.src != "/images/uncategorized.svg") this.src="/images/uncategorized.svg"" title="笔记"></a></li>
		
	</ul>

	<div class="archive-cards">
			<div class="Card-archive" style="display:none">
				<div class="Card-body">
					<h3 class="Card-title">
						<a>
						</a>
					</h3>
					<div class="Card-meta">
						<ul>
							<li><i class="fa fa-calendar"></i> <span class="Card-date"></span></li>
						</ul>
					</div>
				</div>
			</div>
		</div>
</div>

<script src="/libs/fuse.js/dist/fuse.min.js"></script>
<script>
	var options = {
		shouldSort: true,
		threshold: 0.4,
		tokenize: true,
		location: 0,
		distance: 100,
		maxPatternLength: 32,
		minMatchCharLength: 2,
		keys: [
			"title",
			"author",
			"tags"
		]
	};
	var s = '[{"title":"梯子安装以及启用BBR加速","date":"2019-08-21T11:04:12.000Z","content":"安装SSR这个SSR的项目在这里：shadowsocksr\n里面安装和运行都有详细的解释\n\nCentOS 安装\n\n12yum install python-setuptools &amp;&amp; easy_install pippip install shadowsocks\n\nDebian  Ubuntu 安装\n\n12apt-get install python-pippip install shadowsocks\n使用SSR\n前台运行\n\n1ssserver -p 443 -k password -m aes-256-cfb\n\n后台运行\n\n1sudo ssserver -p 443 -k password -m aes-256-cfb --user nobody -d start\n\n停止后台运行\n\n1sudo ssserver -d stop\n\n检查日志\n\n1sudo less varlogshadowsocks.log\n\n前台停止运行用Ctrl+c就可以了，还有一种配置文件的运行方法，感兴趣的可以去上面的网址看看。\n\n问题\n首先，新买的服务器安装后服务器打开但是客服端连不上，这些问题作者在文档里面说的也很清楚。\n\n\n 新买的服务器是没有开启端口的，也就是说防火墙没有把你ssr要用的端口（例如443）打开\n\n\ncentos7版本对防火墙进行 加强,不再使用原来的iptables,启用firewall\n\n123456firewall-cmd --list-ports            查看已开放的端口(默认不开放任何端口)firewall-cmd --zone=public(作用域) --add-port=80tcp(端口和访问类型) --permanent(永久生效)           开启80端口firewall-cmd --reload                  重启防火墙systemctl stop firewalld.service            停止防火墙systemctl disable firewalld.service               禁止防火墙开机启动firewall-cmd --zone= public --remove-port=80tcp --permanent           删除\n\ncentos7以下版本\n\n12345678910开放80，22，8080 端口sbiniptables -I INPUT -p tcp --dport 80 -j ACCEPTsbiniptables -I INPUT -p tcp --dport 22 -j ACCEPTsbiniptables -I INPUT -p tcp --dport 8080 -j ACCEPTetcrc.dinit.diptables save           保存etcinit.diptables status                 查看打开的端口chkconfig iptables on 开启防火墙 永久性生效，重启后不会复原chkconfig iptables off 关闭防火墙 永久性生效，重启后不会复原service iptables start  开启防火墙 即时生效，重启后复原service iptables stop   关闭防火墙 即时生效，重启后复原\n\n选择上面合适的命令将你ssr需要的端口打开\n\n\n打开后重启一下防火墙（firewall-cmd –reload），然后在重启一下服务器（shutdown -r now）\n\n\n然后运行ssr\n\n\n然后先在服务器检测一下端口打开了没（firewall-cmd –list-ports），如果出现你ssr设置的端口说明打开了，如果什么都没有说明没有打开，那你上面的步骤可能有错检查后重新来一遍。\n\n\n服务器检测完后，再在本地检测一下，大家可以去百度一个工具tcping.exe，这个工具可以让我们ping目标IP的端口，下载下来后放在电脑盘的根目录，然后在此目录按住shift点击右键，在出现的菜单中选中’在此处打开命令窗口’这个选项，然后输入\n\n\ntcping IP地址 端口\n\n\n返回 No response 为端口关闭，返回 Port is open 为端口打开状态\n\n\n端口为打开状态时，你就可以用你的ssr客户端链接了，客户端请在上面网址里面去下载，Windows，Android，OS X，iOS都有。\n\n启用bbr以及检测\nBBR是Google开源的一套内核加速算法，可以让你搭建的shadowsocksshadowsocksR速度上一个台阶，本一键搭建ssssr脚本支持一键升级最新版本的内核并开启BBR加速。\nBBR支持4.9以上的，如果低于这个版本则会自动下载最新内容版本的内核后开启BBR加速并重启，如果高于4.9以上则自动开启BBR加速，执行如下脚本命令即可自动开启BBR加速：\nss-flyss-fly.sh -bbr\n装完后需要重启系统，输入y即可立即重启，或者之后输入reboot命令重启。\n判断BBR加速有没有开启成功。输入以下命令：\nsysctl net.ipv4.tcp_available_congestion_control\n如果返回值为：\nnet.ipv4.tcp_available_congestion_control = bbr cubic reno\n后面有bbr，则说明已经开启成功了。\n\n","tags":["Python，ssr"],"path":"2019/08/21/梯子安装以及开启BBR加速/","external_link":""},{"title":"reqeusts+selenium+PIL实现bilibili模拟登陆","date":"2019-11-12T01:48:12.000Z","content":"准备本文介绍一下如何使用python3中的reqeusts+selenium+PIL实现bilibili的模拟登陆，实现该功能的重点在于滑块验证，首先，用到的工具有：\n\npython3环境\nrequests库(可通过pip install requests安装)\nselenium库(可用过pip install selenium安装)\nPIL图片处理库\nchromedriver.exe谷歌浏览器驱动工具(http:npm.taobao.orgmirrorschromedriver)可根据自己的谷歌浏览器版本进行下载\n\n思路进入bilibili登录页面，系统会提示我们输入手机号邮箱和密码，这个我们可以通过selenium很简单的输入，代码如下1234567891011# 获取手机号邮箱输入框username = browser.find_element_by_id(login-username)# 获取密码输入框password = browser.find_element_by_id(login-passwd)# 获取登录按钮login = browser.find_element_by_class_name(btn-login)# 输入用户名和密码，点击登录username.send_keys(手机号邮箱)password.send_keys(密码)login.click()\n在信息输入完毕点击登录之后，会出现滑块验证，验证码如图可以看到，验证码是一张类似拼图的东西，那么能不能找到完整的图片呢？在开发者模式中对网页的构造进行分析之后，我发现如下图所示的两个标签：从命名来看，这应该就是我们要找的完整图片和验证码的图片，接下来只要通过selenium中的screenshot方法将这两种图片截取下来，然后通过PIL库对两张图片进行像素对比就可以得出滑块距离拼图处的水平距离，然后通过selenium将其移动到指定的位置就可以完成自动验证功能。具体实现代码如下：\n12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273import timefrom PIL import Image, ImageChopsfrom selenium import webdriverfrom selenium.webdriver.common.by import Byfrom selenium.webdriver.support.wait import WebDriverWaitfrom selenium.webdriver.support import expected_conditions as ECfrom selenium.webdriver import ActionChainsif __name__ == __main__:    browser = webdriver.Chrome(r....chromedriver.exe)    browser.get(https:passport.bilibili.comlogin)    wait = WebDriverWait(browser, 20)    # 等待用户名，密码，登录按钮加载完毕    username_el = wait.until(EC.presence_of_element_located((By.ID, login-username)))    password_el = wait.until(EC.presence_of_element_located((By.ID, login-passwd)))    login_btn = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, .btn-login)))    # 获取用户名，密码输入框和登录按钮    username = browser.find_element_by_id(login-username)    password = browser.find_element_by_id(login-passwd)    login = browser.find_element_by_class_name(btn-login)    # 输入用户名和密码，点击登录    username.send_keys(手机号邮箱)    password.send_keys(密码)    login.click()    # 等待验证嘛模块加载完毕    chcekel = wait.until(EC.presence_of_element_located((By.CLASS_NAME, geetest_slider_button)))    wait.until(EC.presence_of_element_located((By.CLASS_NAME, geetest_canvas_bg)))    wait.until(EC.presence_of_element_located((By.CLASS_NAME, geetest_canvas_fullbg)))    # 获取滑块    check_btn = browser.find_element_by_class_name(geetest_slider_button)    time.sleep(3)    # 获取验证图片    bg_img = browser.find_element_by_class_name(geetest_canvas_bg)    bg_img.screenshot(bg.png)    # 获取完整图片    browser.execute_script(document.getElementsByClassName(geetest_canvas_fullbg)[0].style=display:block)    fullbg_img = browser.find_element_by_class_name(geetest_canvas_fullbg)    fullbg_img.screenshot(full.png)    # 对比两张图片，找到缺口位置    image_bg = Image.open(bg.png)    image_fullbg = Image.open(full.png)    num = 60  # 定义一个阈值    x = 0    for i in range(50, image_bg.size[0]):        for j in range(image_bg.size[1]):            pixel1 = image_bg.getpixel((i,j))            pixel2 = image_fullbg.getpixel((i,j))            if abs(pixel1[0] - pixel2[0]) &gt; num:                x = i                break    # 滑块滑动    offset_x = [x-50, x-45, x-55]    for i in offset_x:        action_chains = ActionChains(browser)        img = browser.find_element_by_class_name(geetest_window)        action_chains.drag_and_drop_by_offset(check_btn, i, 0).perform()        time.sleep(2)        user_id = browser.get_cookie(DedeUserID)        if user_id:            print(登录成功)            break\n上述代码中，通过像素对比得出的距离会有一定的偏差值，在经过多次的测试之后发现他的偏差值在5个像素左右，所以对其移动的距离+-5组成一个列表，然后循环移动，就可以完成自动登录功能啦。\n","tags":["Python3，爬虫","selenium"],"path":"2019/11/12/reqeusts+selenium实现bilibili模拟登陆/","external_link":""},{"title":"基于scrapy-reids的分布式爬虫爬取智联热门城市Python岗位的招聘信息","date":"2019-08-02T11:28:57.000Z","content":"最近温习了一下scrapy-redis分布式爬虫，所以自己写了一个小例子爬取了智联热门城市的Python岗位招聘信息，所谓分布式爬虫，简单理解就是多台主机一起爬取信息，这里简单说一下原理，当多台主机的爬虫程序启动之后，都会去redis中指定的库中获取请求路径，拿到路径之后就开始爬虫的流程，如果数据库中没有请求路径，则所有的程序都会等待我们放入路径。在使用分布式爬虫之前，最好还是对scrapy爬虫框架有一定的了解。\n准备\npython3环境\nscrapy框架\nscrapy-redis库\n一台or多台主机\nredis数据库(注意：一定要打开远程访问权限，不然别的主机访问不到请求队列)\n\n分析首先我们打开智联，在搜索框中输入python，就会看到很多条招聘信息，但是一定要注意，这些信息是通过ajax加载的，在网页的源代码中是没有的，所以我们不能直接获取网页源代码提取，这里提供两种思路\n1. 通过selenium模拟用户操作，得到网页源码提取数据这种方法理解起来很容易，但是比较麻烦，同时会比较浪费时间，使用selenium请求路径，但是要注意，第一次打开网页的时候会跳出一个模态框，需要点击之后或者按下esc才可以进行下一步操作，可以等待其加载完毕之后通过selenium模拟完成，接着一直模拟点击下一页，切换城市返回源码提取数据即可\n2. 分析ajax链接，直接请求ajax的链接得到json数据，解析数据重点说一下这个方法，在招聘信息页面之后，打开浏览器开发者模式，选择Network，然后选择XHR，就可以看到很多信息，如下图下面的这些信息就是我们需要的东西，然后从这些信息中找到数据所在的接口地址1https:fe-api.zhaopin.comcisou?start=90&amp;pageSize=90&amp;cityId=%s&amp;salary=0,0&amp;workExperience=-1&amp;education=-1&amp;companyType=-1&amp;employmentType=-1&amp;jobWelfareTag=-1&amp;kw=python&amp;kt=3&amp;=0&amp;at=ad352d86a99e49ffb0bec89a8190a5af&amp;rt=0f8efd11216e4d829c7e926ddc7ec9ae&amp;_v=0.13686198&amp;userCode=1014643112&amp;x-zp-page-request-id=1e460842c4264f2ea916f762c5be9382-1573296744354-714218&amp;x-zp-client-id=029d82cf-668a-4293-bd50-978c776e77a4&amp;MmEwMD=4d_mFOVgu9f.ChujeP2YSoZ974TQ0uX0R_7_RzJziMEMCXe4MiDBjnDnrqg0TFSsoLEWwovH6iu8U2pKHuSt4bBtLXGWsbYnZ16JrOP2iibDqsv13HfkqB_l9hW0TFEHsk1coeHyqeqPeaXs0CP2T5EoHqh_EH.Tr._uO3YID1KVHmt6Tccu7hbDxAL2M0aE1jlvYEXPQ2zNOQ7.b082o89Up27K6fv.Qh4CQ6RGUCA0MDaCEt4HndLNZ4pmURLgkzyTCmgOm5kVQJ5MXPZvh7MFwRfqUE_ZLfqOXaFjttS11kblKRqB.B_c8NneirV9RhU.kXRd8vCtWS.ukxIgfOzV1wj5qLLgJsZ2sh0mKbBDFO4VNtGgMbUwMGNCBfskCn41s_OxJQ0S1rqumcVmAQIr4&amp;\n上面这个链接就是我们需要的路径啦，同时一个路径就是一页数据，其中有一些参数不是必须的，所以所以可以省略掉，这个大家可以自己去尝试一下。先着重解释一下几个参数：\n\nstart  每一页数据开始的位置\npageSize  每一页数据的条数\ncityId  城市id\n\n可以看出，每一页有90条数据，那么start参数的值也应该就是90的倍数。\n3. 实现在使用分布式爬虫之前，我们要先需要爬取的地址放到redis的队列里面，以供爬虫能够在队列中拿取到请求路径。上一步中我们已经知道访问api接口可以直接获取到数据，那么就可以写一个Python脚本，将所有热门城市的所有页的api接口存到redis中，具体实现代码如下12345678910111213141516171819202122232425262728293031import redisimport requestsclass AddUrls(object):    def __init__(self, address_list):        self.address_list = address_list        self.r = redis.Redis(host=localhost, port=6379, db=0)    def join_url(self):        headers_dict = &#123;            User-Agent: Mozilla5.0 (Windows NT 6.1; Win64; x64) AppleWebKit537.36 (KHTML, like Gecko) Chrome78.0.3904.70 Safari537.36        &#125;        for i in self.address_list:            start = 0            while True:                url = https:fe-api.zhaopin.comcisou?start=%s&amp;pageSize=90&amp;cityId=%s&amp;salary=0,0&amp;workExperience=-1&amp;education=-1&amp;companyType=-1&amp;employmentType=-1&amp;jobWelfareTag=-1&amp;kw=python&amp;kt=3&amp;=0&amp;at=ad352d86a99e49ffb0bec89a8190a5af&amp;rt=0f8efd11216e4d829c7e926ddc7ec9ae&amp;_v=0.13686198&amp;userCode=1014643112&amp;x-zp-page-request-id=1e460842c4264f2ea916f762c5be9382-1573296744354-714218&amp;x-zp-client-id=029d82cf-668a-4293-bd50-978c776e77a4&amp;MmEwMD=4d_mFOVgu9f.ChujeP2YSoZ974TQ0uX0R_7_RzJziMEMCXe4MiDBjnDnrqg0TFSsoLEWwovH6iu8U2pKHuSt4bBtLXGWsbYnZ16JrOP2iibDqsv13HfkqB_l9hW0TFEHsk1coeHyqeqPeaXs0CP2T5EoHqh_EH.Tr._uO3YID1KVHmt6Tccu7hbDxAL2M0aE1jlvYEXPQ2zNOQ7.b082o89Up27K6fv.Qh4CQ6RGUCA0MDaCEt4HndLNZ4pmURLgkzyTCmgOm5kVQJ5MXPZvh7MFwRfqUE_ZLfqOXaFjttS11kblKRqB.B_c8NneirV9RhU.kXRd8vCtWS.ukxIgfOzV1wj5qLLgJsZ2sh0mKbBDFO4VNtGgMbUwMGNCBfskCn41s_OxJQ0S1rqumcVmAQIr4&amp; % (start, i)                result = requests.get(url, headers=headers_dict).json()                start += 90                if result[data][results]:                    self.r.lpush(zhilian, url)                else:                    breakif __name__ == __main__:    address_list = [703, 530, 538, 765, 763, 531, 801, 653, 736, 600, 613, 664, 773,635, 702, 639, 599, 854, 719, 749, 551, 622, 636, 654, 681, 682, 565]    add = AddUrls(address_list)    add.join_url()\n爬虫实现代码如下：\n爬虫文件zl.py1234567891011121314151617181920212223242526272829303132333435363738394041424344import jsonimport scrapyfrom scrapy import Requestfrom scrapy.http import HtmlResponsefrom scrapy_redis.spiders import RedisSpiderfrom ZhilProject.items import ZhilprojectItemclass ZlSpider(RedisSpider):    name = zl    allowed_domains = [zhaopin.com]    # start_urls = [https:sou.zhaopin.com?jl=703&amp;sf=0&amp;st=0&amp;kw=python&amp;kt=3]    redis_key = zhilian    def parse(self, response: HtmlResponse):        data = json.loads(response.text)        for i in data[data][results]:            item = ZhilprojectItem()            try:                item[number] = i[number]                item[position] = i[jobName]                item[salary] = i[salary]                item[address] = i[city][items][0][name]                item[date] = i[updateDate]                item[experience] = i[workingExp][name]                item[education] = i[eduLevel][name]                item[corporate_name] = i[company][name]                item[c_type] = i[company][type][name]                item[scale] = i[company][size][name]                yield item            except:                item[number] =                 item[position] =                 item[salary] =                 item[address] =                 item[date] =                 item[experience] =                 item[education] =                 item[corporate_name] =                 item[c_type] =                 item[scale] =                 yield item\nitems.py1234567891011121314151617181920212223242526272829303132333435363738394041424344import jsonimport scrapyfrom scrapy import Requestfrom scrapy.http import HtmlResponsefrom scrapy_redis.spiders import RedisSpiderfrom ZhilProject.items import ZhilprojectItemclass ZlSpider(RedisSpider):    name = zl    allowed_domains = [zhaopin.com]    # start_urls = [https:sou.zhaopin.com?jl=703&amp;sf=0&amp;st=0&amp;kw=python&amp;kt=3]    redis_key = zhilian    def parse(self, response: HtmlResponse):        data = json.loads(response.text)        for i in data[data][results]:            item = ZhilprojectItem()            try:                item[number] = i[number]                item[position] = i[jobName]                item[salary] = i[salary]                item[address] = i[city][items][0][name]                item[date] = i[updateDate]                item[experience] = i[workingExp][name]                item[education] = i[eduLevel][name]                item[corporate_name] = i[company][name]                item[c_type] = i[company][type][name]                item[scale] = i[company][size][name]                yield item            except:                item[number] =                 item[position] =                 item[salary] =                 item[address] =                 item[date] =                 item[experience] =                 item[education] =                 item[corporate_name] =                 item[c_type] =                 item[scale] =                 yield item\npipelines.py1234567891011121314151617181920212223242526272829303132import pymysqlclass ZhilprojectPipeline(object):    def __init__(self):        self.conn = pymysql.Connect(            host=localhost,            port=3306,            user=数据库用户名,            password=数据库密码,            database=zhilian,            charset=utf8        )    def process_item(self, item, spider):        cursor = self.conn.cursor()        sql = insert into python(number, job_name, corporate_name, salary, date, address, experience, education, c_type, scale) values (&#123;&#125;, &#123;&#125;, &#123;&#125;, &#123;&#125;, &#123;&#125;, &#123;&#125;, &#123;&#125;, &#123;&#125;, &#123;&#125;, &#123;&#125;).format(            item[number], item[position], item[corporate_name], item[salary], item[date], item[address], item[experience], item[education], item[c_type], item[scale]        )        try:            cursor.execute(sql)            self.conn.commit()            print(插入成功)        except Exception as e:            print(插入失败：, e)            self.conn.rollback()        cursor.close()        return item    def close_spider(self, spider):        self.conn.close()\nsettings.py1234567891011121314151617181920212223# -*- coding: utf-8 -*-BOT_NAME = ZhilProjectSPIDER_MODULES = [ZhilProject.spiders]NEWSPIDER_MODULE = ZhilProject.spidersUSER_AGENT = Mozilla5.0 (Windows NT 6.1; Win64; x64) AppleWebKit537.36 (KHTML, like Gecko) Chrome78.0.3904.70 Safari537.36ROBOTSTXT_OBEY = FalseDOWNLOAD_DELAY = 3ITEM_PIPELINES = &#123;    ZhilProject.pipelines.ZhilprojectPipeline: 300,&#125;SCHEDULER = scrapy_redis.scheduler.Scheduler  # 该调度器，可以使请求队列放在redis中DUPEFILTER_CLASS = scrapy_redis.dupefilter.RFPDupeFilter  # 去重配置SCHEDULER_PERSIST = True  # 持久化，不清理redis队列REDIS_HOST = localhostREDIS_PORT = 6379\n注意！这是Master端的配置文件，Slaver端的配置文件需要将redis的地址修改为Master端的ip\n配置完成之后，找至少两台主机先运行之前的脚本将请求路径存到redis中去，然后启动Master端与Slaver端，就可以实现分布式爬虫啦！\n","tags":["Python3，爬虫","scrapy-redis"],"path":"2019/08/02/基于scrapy-reids的分布式爬虫爬取智联热门城市Python岗位的招聘信息/","external_link":""}]';

	s = s.replace(/\\n/g, "\\n")
               .replace(/\\'/g, "\\'")
               .replace(/\\"/g, '\\"')
               .replace(/\\&/g, "\\&")
               .replace(/\\r/g, "\\r")
               .replace(/\\t/g, "\\t")
               .replace(/\\b/g, "\\b")
               .replace(/\\f/g, "\\f")

// remove non-printable and other non-valid JSON chars
	s = s.replace(/[\u0000-\u0019]+/g,"");
	var list = JSON.parse(s);
	var fuse = new Fuse(list, options);
	var el = document.getElementById('search-form');
	var newBox = $('.Card-archive').first().clone();
	el.oninput = function(event){
		var searchText = el.value;
		var result = fuse.search(searchText);
		$('.archive-cards .Card-archive').remove();
		for(var i in result){
			var anotherBox = newBox.clone();
			var dateStr = new Date(result[i].date);
			anotherBox.css('display','flex');
			var url = "";
			if(result[i].external_link !== ""){
				url = result[i].external_link;
			}else{
				url = '/' + result[i].path;
			}

			anotherBox.find('.Card-title a').text(result[i].title).attr('href', url);
			anotherBox.find('.Card-date').text(dateStr.toDateString());
			anotherBox.appendTo('.archive-cards');
		}
	}
</script>

<div class="tagcloud-container">
<div class="tag-cloud">
	<a href="/tags/Python3，爬虫/" style="font-size: 2em; color: #d63e0a">Python3，爬虫</a> <a href="/tags/Python，ssr/" style="font-size: 0.8em; color: #488baf">Python，ssr</a> <a href="/tags/scrapy-redis/" style="font-size: 0.8em; color: #488baf">scrapy-redis</a> <a href="/tags/selenium/" style="font-size: 0.8em; color: #488baf">selenium</a>
</div>
</div>

  </div>

  

<footer id="footer">
    <div class="footer-copyright">
        <div>
            <p> Copyright by <a href>ZhaoJian </a> @ 2019</p>
            <p>Designed by: <i class="fas fa-paint-brush"></i> <a href="#">ZhaoJian</a> &bull; Powered by <a href="http://hexo.io">Hexo.</a></p>
        </div>
    </div>
    
    <div class="footer-social">
        
            
                
                    <div class="footer-social-item"><a href="https://github.com/benjaminzj" target="_blank"><i class="fab fa-github fa-2x" aria-hidden="true"></i></a></div>
                
            
        
    </div>
</footer>

<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js">
</script>

  <br>

  <div id="footer-nav" class='footer-nav'>
		



<nav id="nav">
	
	
		
			
				
			
		
		
	
		
			
		
		
	
	
	<div class="nav-item" id="nav-item-archive">
		
				<div class="nav-icon">
				
			<a href="/archives/" title="归档">
			<img src="/images/icons/blue-shadow/archive.svg" alt>
			</a>
		</div>
	</div>
	<div class="nav-item" id="nav-item-search">
		
		<div class="nav-icon active_dot">
		
			<a href="/search/" title="搜索">
			<img src="/images/icons/blue-shadow/search.svg" alt>
			</a>
		</div>
	</div>
	<div class="nav-item" id="nav-item-more">
		<div class="nav-icon">
				<a href="#" onclick="onClickMenuIcon(event);" ontouchstart="onClickMenuIcon(event);">
				<img src="/images/icons/blue-shadow/menu.svg" alt>
				</a>
		</div>
		<div class="nav-more-menu">
				<i class="far fa-times-circle" id="nav-more-menu-close" onclick="onClickNavMenuClose(event);" ontouchstart="onClickNavMenuClose(event);"></i>
		
			
			
				
			
		
			<div class="nav-more-item">
					<div class="nav-name">
						<a class="nav-link" href="/categories/项目/">
							<span>项目</span>
						</a>
					</div>
			</div>
		
		
			
			
				
					
				
			
		
		
		
		<div class="nav-more-item">
				<div class="nav-name">
					<a class="nav-link" href="/categories/笔记/">
						<span>笔记</span>
					</a>
				</div>
		</div>
		
	</div>
	</div>
</nav>

	</div>

  



    <!-- 来必力City版安装代码 -->

<script type="text/javascript">
   (function(d, s) {
       var j, e = d.getElementsByTagName(s)[0];

       if (typeof LivereTower === 'function') { return; }

       j = d.createElement(s);
       j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
       j.async = true;

       e.parentNode.insertBefore(j, e);
   })(document, 'script');
</script>
<noscript>为正常使用来必力评论功能请激活JavaScript</noscript>

<!-- City版安装代码已完成 -->





    <script src="/js/lightgallery.min.js"></script>
<script src="/js/lg-zoom.min.js"></script>
<script type="text/javascript">
    $(document).ready(function() {
        $("#lightgallery").lightGallery(); 
        $(".article-content img").each(function(){
            console.log($(this).attr('src'))
            $(this).attr('data-src', $(this).attr('src')).lightGallery({
                selector: 'this'
            })
        });
    });
</script>






<script type="text/javascript">

  
</script>



<!-- <script src="/js/post.js"></script> -->

<script src="/js/headroom.min.js"></script>

<script data-no-instant type="text/javascript">

initHeadroom();

changeLayoutOnTouchScreen();

// 
</script>


<!-- End Document
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
<script src="/live2dw/lib/L2Dwidget.min.js?0c58a1486de42ac6cc1c59c7d98ae887"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/01/assets/haru01.model.json"},"display":{"position":"right","width":100,"height":200,"hOffset":5,"vOffset":0},"mobile":{"show":true},"log":false});</script></body>
</html>
